{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The experiment on real-world ICH detection using mechanism learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import causalBootstrapping as cb\n",
    "from distEst_lib import MultivarContiDistributionEstimator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import os \n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set random seeds for reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "rng_bootstrap = np.random.RandomState(42)\n",
    "rng_train_test = np.random.RandomState(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_read(dir_list, img_size):\n",
    "    img_list = []\n",
    "    for dir in dir_list:\n",
    "        img = cv2.imread(dir, cv2.IMREAD_GRAYSCALE)\n",
    "        img = cv2.resize(img, img_size)\n",
    "        img_list.append(img.flatten())\n",
    "    return np.array(img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = r\"../../test_data/ICH_data/\"\n",
    "effect_dir = dir + r\"ct_clean/\"\n",
    "mediator_dir = dir\n",
    "cause_dir = dir\n",
    "imgs_names = os.listdir(effect_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_names = sorted(imgs_names, key=lambda x: int(x.split('.')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_imgs = img_read([effect_dir + img_name for img_name in imgs_names], (128, 128))\n",
    "cause_table = pd.read_csv(cause_dir + \"hemorrhage_diagnosis_ct_clean.csv\")\n",
    "mediator_table = pd.read_csv(mediator_dir + \"mediator_embedding.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cause Var Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {0: \"No Hemorrhage\", 1: \"Intraventricular\", 2: \"Intraparenchymal\", 3: \"Subarachnoid\", 4: \"Epidural\", 5: \"Subdural\"}\n",
    "\n",
    "cause_table[\"category\"] = np.nan\n",
    "cause_table.loc[cause_table[\"No_Hemorrhage\"] == 1, \"category\"] = 0\n",
    "cause_table.loc[cause_table[\"Intraventricular\"] == 1, \"category\"] = 1\n",
    "cause_table.loc[cause_table[\"Intraparenchymal\"] == 1, \"category\"] = 2\n",
    "cause_table.loc[cause_table[\"Subarachnoid\"] == 1, \"category\"] = 3\n",
    "cause_table.loc[cause_table[\"Epidural\"] == 1, \"category\"] = 4\n",
    "cause_table.loc[cause_table[\"Subdural\"] == 1, \"category\"] = 5\n",
    "cause_table[\"category\"] = cause_table[\"category\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cause_category = cause_table[\"category\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_class = len(cause_table[\"category\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mediator Var Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.445242</td>\n",
       "      <td>11.299638</td>\n",
       "      <td>8.707513</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.309047</td>\n",
       "      <td>7.546306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.398108</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.386361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7.445242</td>\n",
       "      <td>11.299638</td>\n",
       "      <td>8.707513</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.309047</td>\n",
       "      <td>7.546306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.398108</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.386361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.445242</td>\n",
       "      <td>11.299638</td>\n",
       "      <td>8.707513</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.309047</td>\n",
       "      <td>7.546306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.398108</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.386361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.445242</td>\n",
       "      <td>11.299638</td>\n",
       "      <td>8.707513</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.309047</td>\n",
       "      <td>7.546306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.398108</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.386361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.445242</td>\n",
       "      <td>11.299638</td>\n",
       "      <td>8.707513</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.309047</td>\n",
       "      <td>7.546306</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.398108</td>\n",
       "      <td>0.0</td>\n",
       "      <td>12.386361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0          1         2    3         4         5    6          7  \\\n",
       "0  7.445242  11.299638  8.707513  0.0  3.309047  7.546306  0.0  10.398108   \n",
       "1  7.445242  11.299638  8.707513  0.0  3.309047  7.546306  0.0  10.398108   \n",
       "2  7.445242  11.299638  8.707513  0.0  3.309047  7.546306  0.0  10.398108   \n",
       "3  7.445242  11.299638  8.707513  0.0  3.309047  7.546306  0.0  10.398108   \n",
       "4  7.445242  11.299638  8.707513  0.0  3.309047  7.546306  0.0  10.398108   \n",
       "\n",
       "     8          9  \n",
       "0  0.0  12.386361  \n",
       "1  0.0  12.386361  \n",
       "2  0.0  12.386361  \n",
       "3  0.0  12.386361  \n",
       "4  0.0  12.386361  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mediator_table.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3\n",
       "0.0    2411\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mediator_table['3'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the all-zero column\n",
    "mediator_table.drop(columns=['3'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Causal Bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Format variables and normalize the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "effect_imgs = effect_imgs/255.0\n",
    "cause_category = cause_category.reshape(-1,1)\n",
    "mediaor_values = mediator_table.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estimate required distributions for causal bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cause_data = {\"Y\": cause_category}\n",
    "mediator_data = {\"Z\": mediaor_values}\n",
    "effect_data = {\"X\": effect_imgs} \n",
    "n_bins_yz = [0]+[6 for i in range(mediaor_values.shape[1])]\n",
    "n_bins_y = [0]\n",
    "\n",
    "joint_yz_data = np.concatenate((cause_category, mediaor_values), axis = 1)\n",
    "\n",
    "dist_estimator_yz = MultivarContiDistributionEstimator(data_fit=joint_yz_data, n_bins = n_bins_yz)\n",
    "pdf_yz, pyz = dist_estimator_yz.fit_histogram()\n",
    "dist_estimator_y = MultivarContiDistributionEstimator(data_fit=cause_category, n_bins = n_bins_y)\n",
    "pdf_y, py = dist_estimator_y.fit_histogram()\n",
    "\n",
    "dist_map = {\"Y,Z\": lambda Y, Z: pdf_yz([Y,Z]),\n",
    "            \"Y',Z\": lambda Y_prime, Z: pdf_yz([Y_prime,Z]),\n",
    "            \"Y\": lambda Y: pdf_y(Y),\n",
    "            \"Y'\": lambda Y_prime: pdf_y(Y_prime)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare inputs for causal bootstrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interventional prob.:p_{Y}(X)=\\sum_{Z,Y'}[p(X|Z,Y')p(Z|Y)p(Y')]\n",
      "Causal bootstrapping weights function: [P(Y,Z)P(Y')]/N*[P(Y)P(Y',Z)]\n",
      "Required distributions:\n",
      "1: P(Y,Z)\n",
      "2: P(Y')\n",
      "3: P(Y)\n",
      "4: P(Y',Z)\n"
     ]
    }
   ],
   "source": [
    "causal_graph = '\"Front-door\"; \\\n",
    "                Y; X; Z; \\\n",
    "                Y -> Z; \\\n",
    "                Z -> X; \\\n",
    "                X <-> Y;'\n",
    "weight_func_lam, weight_func_str = cb.general_cb_analysis(causal_graph = causal_graph, \n",
    "                                                          effect_var_name = 'X', \n",
    "                                                          cause_var_name = 'Y',\n",
    "                                                          info_print = True)\n",
    "N = cause_category.shape[0]\n",
    "w_func = weight_func_lam(dist_map = dist_map, N = N, kernel = None)\n",
    "cause_var_name = list(cause_data.keys())[0]\n",
    "effect_var_name = list(effect_data.keys())[0]\n",
    "mediator_var_name = list(mediator_data.keys())[0]\n",
    "\n",
    "data = {cause_var_name+\"'\": list(cause_data.values())[0]}\n",
    "data.update(effect_data)\n",
    "data.update(mediator_data)\n",
    "n_sample = [1000, 800, 800, 800, 800, 800]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the training and test datasets by identifying the causal bootstrap indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainVal_X_deconf = np.empty((0, effect_imgs.shape[1]))\n",
    "trainVal_Y_deconf = np.empty((0, 1))\n",
    "test_X_deconf = np.empty((0, effect_imgs.shape[1]))\n",
    "test_Y_deconf = np.empty((0, 1))\n",
    "\n",
    "trainVal_X_conf = np.empty((0, effect_imgs.shape[1]))\n",
    "trainVal_Y_conf = np.empty((0, 1))\n",
    "test_X_conf = np.empty((0, effect_imgs.shape[1]))\n",
    "test_Y_conf = np.empty((0, 1))\n",
    "\n",
    "train_size = 0.85\n",
    "weights = np.empty((n_class, N))\n",
    "idx_ib_by_class = []\n",
    "idx_ib_all_unique = []\n",
    "for class_i in range(n_class):\n",
    "    weights_itv = cb.weight_compute(w_func, data, intv_var = {\"Y\":[class_i for i in range(N)]})\n",
    "    # Causal Bootstrapping weights\n",
    "    weights[class_i] = weights_itv\n",
    "    weights_norm = weights_itv/np.sum(weights_itv)\n",
    "    idx_ib = rng_bootstrap.choice(range(N), size = n_sample[class_i], replace = True, p = weights_norm)\n",
    "    idx_ib_by_class.append(list(idx_ib))\n",
    "    idx_ib_unique = np.unique(idx_ib)\n",
    "    idx_ib_all_unique.append(idx_ib_unique)\n",
    "idx_ib_all_unique = np.unique(np.concatenate(idx_ib_all_unique))\n",
    "idx_ib_train = rng_train_test.choice(idx_ib_all_unique, size = int(train_size*idx_ib_all_unique.shape[0]), replace = False)\n",
    "idx_ib_test = np.setdiff1d(idx_ib_all_unique, idx_ib_train)\n",
    "idx_all_test = np.setdiff1d(range(N), idx_ib_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for class_i in range(n_class):\n",
    "    idx_ib_train_bootstrap = [idx for idx in idx_ib_by_class[class_i] if idx in idx_ib_train]\n",
    "    trainVal_X_deconf = np.concatenate((trainVal_X_deconf, effect_imgs[idx_ib_train_bootstrap, :]), axis = 0)\n",
    "    trainVal_Y_deconf = np.concatenate((trainVal_Y_deconf, np.array([class_i for i in range(len(idx_ib_train_bootstrap))]).reshape(-1,1)), axis = 0)\n",
    "    \n",
    "    idx_ib_test_bootstrap = [idx for idx in idx_ib_by_class[class_i] if idx in idx_ib_test]\n",
    "    test_X_deconf = np.concatenate((test_X_deconf, effect_imgs[idx_ib_test_bootstrap, :]), axis = 0)\n",
    "    test_Y_deconf = np.concatenate((test_Y_deconf, np.array([class_i for i in range(len(idx_ib_test_bootstrap))]).reshape(-1,1)), axis = 0)\n",
    "    \n",
    "trainVal_X_conf = effect_imgs[idx_ib_train, :]\n",
    "trainVal_Y_conf = cause_category[idx_ib_train].reshape(-1,1)\n",
    "test_X_conf = effect_imgs[idx_all_test, :]\n",
    "test_Y_conf = cause_category[idx_all_test].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resample the confounded datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample the training set\n",
    "n_rus_train = [np.min([trainVal_Y_deconf[trainVal_Y_deconf == i].shape[0],trainVal_Y_conf[trainVal_Y_conf == i].shape[0]]) for i in range(n_class)]\n",
    "rus_train = RandomUnderSampler(sampling_strategy={i: n_rus_train[i] for i in range(n_class)}, random_state = 42)\n",
    "trainVal_X_conf, trainVal_Y_conf = rus_train.fit_resample(trainVal_X_conf, trainVal_Y_conf)\n",
    "\n",
    "n_ros_train = [trainVal_Y_deconf[trainVal_Y_deconf == i].shape[0] for i in range(n_class)]\n",
    "ros_train = RandomOverSampler(sampling_strategy = {i: n_ros_train[i] for i in range(n_class)}, random_state = 42)\n",
    "trainVal_X_conf, trainVal_Y_conf = ros_train.fit_resample(trainVal_X_conf, trainVal_Y_conf)\n",
    "\n",
    "# Resample the test set\n",
    "n_rus_test = [np.min([test_Y_deconf[test_Y_deconf == i].shape[0],test_Y_conf[test_Y_conf == i].shape[0]]) for i in range(6)]\n",
    "rus_test = RandomUnderSampler(sampling_strategy={i: n_rus_test[i] for i in range(n_class)}, random_state = 42)\n",
    "test_X_conf, test_Y_conf = rus_test.fit_resample(test_X_conf, test_Y_conf)\n",
    "\n",
    "n_ros_test = [test_Y_deconf[test_Y_deconf == i].shape[0] for i in range(n_class)]\n",
    "ros_test = RandomOverSampler(sampling_strategy = {i: n_ros_test[i] for i in range(n_class)}, random_state = 42)\n",
    "test_X_conf, test_Y_conf = ros_test.fit_resample(test_X_conf, test_Y_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split the validation set for both datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deconfounded data split\n",
    "val_size = 0.1\n",
    "train_X_deconf, val_X_deconf, train_Y_deconf, val_Y_deconf = train_test_split(trainVal_X_deconf, trainVal_Y_deconf, \n",
    "                                                                              test_size=val_size, stratify=trainVal_Y_deconf, random_state=17)\n",
    "# Confounded data split\n",
    "train_X_conf, val_X_conf, train_Y_conf, val_Y_conf = train_test_split(trainVal_X_conf, trainVal_Y_conf,\n",
    "                                                                      test_size=val_size, stratify=trainVal_Y_conf, random_state=17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De-confounded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 128, 128, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 128, 128, 16  800         ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 64, 64, 16)   0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 64, 64, 32)   12832       ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 32, 32, 32)  0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 32, 32, 64)   18496       ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " average_pooling2d (AveragePool  (None, 16, 16, 64)  0           ['conv2d_2[0][0]']               \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " average_pooling2d_1 (AveragePo  (None, 16, 16, 1)   0           ['input_1[0][0]']                \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 16, 16, 64)   0           ['average_pooling2d[0][0]',      \n",
      "                                                                  'average_pooling2d_1[0][0]']    \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 16, 16, 64)   0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 16, 16, 16)   1040        ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 4096)         0           ['conv2d_3[0][0]']               \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 4096)         0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           262208      ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 64)           0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 64)           4160        ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 64)           0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 32)           2080        ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 6)            198         ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 301,814\n",
      "Trainable params: 301,814\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def resNetCNN_model(input_shape, num_class):\n",
    "    input_img = layers.Input(shape=input_shape)\n",
    "    \n",
    "    short_cut = input_img\n",
    "    x = layers.Conv2D(16, (7, 7), activation='relu', padding='same')(input_img)\n",
    "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    \n",
    "    x = layers.Conv2D(32, (5, 5), activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    \n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.AveragePooling2D((2, 2), padding='same')(x)\n",
    "    \n",
    "    short_cut = layers.AveragePooling2D((8, 8), padding='same')(short_cut)\n",
    "    x = layers.Add()([x, short_cut])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    x = layers.Conv2D(16, (1, 1), activation='relu', padding='same')(x)\n",
    "    \n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    \n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    \n",
    "    encoded = layers.Dense(num_class, activation='softmax')(x)\n",
    "    \n",
    "    return models.Model(input_img, encoded)\n",
    "\n",
    "model_deconf = resNetCNN_model((128, 128, 1), n_class)\n",
    "model_deconf.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "model_deconf.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the deconfounded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "950/950 [==============================] - 12s 5ms/step - loss: 1.1509 - accuracy: 0.5529 - val_loss: 0.6909 - val_accuracy: 0.7701\n",
      "Epoch 2/75\n",
      "950/950 [==============================] - 4s 4ms/step - loss: 0.6024 - accuracy: 0.7886 - val_loss: 0.3862 - val_accuracy: 0.9005\n",
      "Epoch 3/75\n",
      "950/950 [==============================] - 4s 4ms/step - loss: 0.4385 - accuracy: 0.8452 - val_loss: 0.2780 - val_accuracy: 0.9171\n",
      "Epoch 4/75\n",
      "950/950 [==============================] - 4s 4ms/step - loss: 0.3822 - accuracy: 0.8710 - val_loss: 0.2715 - val_accuracy: 0.9147\n",
      "Epoch 5/75\n",
      "950/950 [==============================] - 4s 5ms/step - loss: 0.2944 - accuracy: 0.9049 - val_loss: 0.2361 - val_accuracy: 0.8957\n",
      "Epoch 6/75\n",
      "950/950 [==============================] - 4s 5ms/step - loss: 0.3067 - accuracy: 0.8963 - val_loss: 0.2468 - val_accuracy: 0.9171\n",
      "Epoch 7/75\n",
      "950/950 [==============================] - 4s 4ms/step - loss: 0.2661 - accuracy: 0.9105 - val_loss: 0.2618 - val_accuracy: 0.9100\n",
      "Epoch 8/75\n",
      "950/950 [==============================] - 4s 4ms/step - loss: 0.2462 - accuracy: 0.9200 - val_loss: 0.2208 - val_accuracy: 0.9242\n",
      "Epoch 9/75\n",
      "950/950 [==============================] - 4s 4ms/step - loss: 0.2207 - accuracy: 0.9289 - val_loss: 0.2320 - val_accuracy: 0.9242\n",
      "Epoch 10/75\n",
      "950/950 [==============================] - 4s 4ms/step - loss: 0.2333 - accuracy: 0.9250 - val_loss: 0.1761 - val_accuracy: 0.9360\n",
      "Epoch 11/75\n",
      "950/950 [==============================] - 4s 4ms/step - loss: 0.2055 - accuracy: 0.9360 - val_loss: 0.1999 - val_accuracy: 0.9360\n",
      "Epoch 12/75\n",
      "950/950 [==============================] - 4s 4ms/step - loss: 0.1892 - accuracy: 0.9363 - val_loss: 0.1755 - val_accuracy: 0.9360\n",
      "Epoch 13/75\n",
      "950/950 [==============================] - 4s 4ms/step - loss: 0.1851 - accuracy: 0.9405 - val_loss: 0.1635 - val_accuracy: 0.9455\n",
      "Epoch 14/75\n",
      "950/950 [==============================] - 4s 4ms/step - loss: 0.1780 - accuracy: 0.9376 - val_loss: 0.1892 - val_accuracy: 0.9384\n",
      "Epoch 15/75\n",
      "950/950 [==============================] - 4s 4ms/step - loss: 0.1795 - accuracy: 0.9450 - val_loss: 0.1749 - val_accuracy: 0.9336\n",
      "Epoch 16/75\n",
      "950/950 [==============================] - 4s 4ms/step - loss: 0.1734 - accuracy: 0.9458 - val_loss: 0.1668 - val_accuracy: 0.9360\n",
      "Epoch 17/75\n",
      "950/950 [==============================] - 4s 4ms/step - loss: 0.1789 - accuracy: 0.9413 - val_loss: 0.1583 - val_accuracy: 0.9431\n",
      "Epoch 18/75\n",
      "950/950 [==============================] - 4s 4ms/step - loss: 0.1634 - accuracy: 0.9471 - val_loss: 0.2349 - val_accuracy: 0.9336\n",
      "Epoch 19/75\n",
      "950/950 [==============================] - 4s 4ms/step - loss: 0.1615 - accuracy: 0.9484 - val_loss: 0.2230 - val_accuracy: 0.9384\n",
      "Epoch 20/75\n",
      "950/950 [==============================] - 4s 4ms/step - loss: 0.1667 - accuracy: 0.9439 - val_loss: 0.1913 - val_accuracy: 0.9431\n",
      "Epoch 21/75\n",
      "950/950 [==============================] - 4s 4ms/step - loss: 0.1589 - accuracy: 0.9505 - val_loss: 0.2250 - val_accuracy: 0.9408\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x203c1ecad60>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_Y_deconf_oh = to_categorical(train_Y_deconf.reshape(-1), num_classes=6)\n",
    "val_Y_deconf_oh = to_categorical(val_Y_deconf.reshape(-1), num_classes=6)\n",
    "early_stopping=EarlyStopping(monitor='val_accuracy', min_delta=0,\n",
    "                            patience=8, verbose=0, mode='max',\n",
    "                            baseline=None, restore_best_weights=True)\n",
    "\n",
    "model_deconf.fit(train_X_deconf.reshape(-1, 128, 128, 1), train_Y_deconf_oh, \n",
    "                 epochs=75, batch_size=4, shuffle=True,\n",
    "                 validation_data=(val_X_deconf.reshape(-1, 128, 128, 1), val_Y_deconf_oh),\n",
    "                 callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 1s 4ms/step\n",
      "25/25 [==============================] - 0s 6ms/step\n",
      "132/132 [==============================] - 0s 4ms/step\n",
      "25/25 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "Y_pred_conf_train = model_deconf.predict(trainVal_X_conf.reshape(-1, 128, 128, 1))\n",
    "Y_pred_conf_test = model_deconf.predict(test_X_conf.reshape(-1, 128, 128, 1))\n",
    "\n",
    "Y_pred_deconf_train = model_deconf.predict(trainVal_X_deconf.reshape(-1, 128, 128, 1))\n",
    "Y_pred_deconf_test = model_deconf.predict(test_X_deconf.reshape(-1, 128, 128, 1))\n",
    "\n",
    "Y_pred_conf_train = np.argmax(Y_pred_conf_train, axis=1)\n",
    "Y_pred_conf_test = np.argmax(Y_pred_conf_test, axis=1)\n",
    "Y_pred_deconf_train = np.argmax(Y_pred_deconf_train, axis=1)\n",
    "Y_pred_deconf_test = np.argmax(Y_pred_deconf_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on confounded test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.96      0.93       152\n",
      "           1       0.97      1.00      0.99       269\n",
      "           2       1.00      0.72      0.83        95\n",
      "           3       0.79      1.00      0.88        74\n",
      "           4       0.95      0.84      0.89       100\n",
      "           5       0.98      1.00      0.99        90\n",
      "\n",
      "    accuracy                           0.94       780\n",
      "   macro avg       0.93      0.92      0.92       780\n",
      "weighted avg       0.94      0.94      0.94       780\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_Y_conf.reshape(-1), Y_pred_conf_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on deconfounded test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.93      0.92       152\n",
      "         1.0       0.92      1.00      0.96       269\n",
      "         2.0       1.00      0.52      0.68        95\n",
      "         3.0       0.72      0.46      0.56        74\n",
      "         4.0       0.84      0.85      0.85       100\n",
      "         5.0       0.67      1.00      0.80        90\n",
      "\n",
      "    accuracy                           0.86       780\n",
      "   macro avg       0.84      0.79      0.79       780\n",
      "weighted avg       0.87      0.86      0.85       780\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_Y_deconf.reshape(-1), Y_pred_deconf_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confounded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 128, 128, 1  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 128, 128, 16  800         ['input_2[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 64, 64, 16)  0           ['conv2d_4[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 64, 64, 32)   12832       ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling2d_3 (MaxPooling2D)  (None, 32, 32, 32)  0           ['conv2d_5[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_6 (Conv2D)              (None, 32, 32, 64)   18496       ['max_pooling2d_3[0][0]']        \n",
      "                                                                                                  \n",
      " average_pooling2d_2 (AveragePo  (None, 16, 16, 64)  0           ['conv2d_6[0][0]']               \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " average_pooling2d_3 (AveragePo  (None, 16, 16, 1)   0           ['input_2[0][0]']                \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 16, 16, 64)   0           ['average_pooling2d_2[0][0]',    \n",
      "                                                                  'average_pooling2d_3[0][0]']    \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 16, 16, 64)   0           ['add_1[0][0]']                  \n",
      "                                                                                                  \n",
      " conv2d_7 (Conv2D)              (None, 16, 16, 16)   1040        ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 4096)         0           ['conv2d_7[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 4096)         0           ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 64)           262208      ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 64)           0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 64)           4160        ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 64)           0           ['dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 32)           2080        ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 6)            198         ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 301,814\n",
      "Trainable params: 301,814\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "train_Y_oh_conf = to_categorical(train_Y_conf, num_classes=6)\n",
    "val_Y_oh_conf = to_categorical(val_Y_conf.reshape(-1), num_classes=6)\n",
    "\n",
    "model_conf = resNetCNN_model((128, 128, 1), n_class)\n",
    "model_conf.compile(optimizer='adam',\n",
    "                loss='categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "model_conf.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the confounded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n",
      "950/950 [==============================] - 6s 6ms/step - loss: 1.0660 - accuracy: 0.5766 - val_loss: 0.3364 - val_accuracy: 0.8720\n",
      "Epoch 2/75\n",
      "950/950 [==============================] - 4s 4ms/step - loss: 0.4409 - accuracy: 0.8339 - val_loss: 0.1841 - val_accuracy: 0.9313\n",
      "Epoch 3/75\n",
      "950/950 [==============================] - 4s 4ms/step - loss: 0.3209 - accuracy: 0.8807 - val_loss: 0.1369 - val_accuracy: 0.9479\n",
      "Epoch 4/75\n",
      "950/950 [==============================] - 4s 4ms/step - loss: 0.2445 - accuracy: 0.9147 - val_loss: 0.1123 - val_accuracy: 0.9645\n",
      "Epoch 5/75\n",
      "950/950 [==============================] - 4s 4ms/step - loss: 0.1833 - accuracy: 0.9363 - val_loss: 0.0822 - val_accuracy: 0.9763\n",
      "Epoch 6/75\n",
      "950/950 [==============================] - 4s 4ms/step - loss: 0.1784 - accuracy: 0.9405 - val_loss: 0.0859 - val_accuracy: 0.9787\n",
      "Epoch 7/75\n",
      "950/950 [==============================] - 4s 4ms/step - loss: 0.1177 - accuracy: 0.9555 - val_loss: 0.0659 - val_accuracy: 0.9810\n",
      "Epoch 8/75\n",
      "950/950 [==============================] - 4s 4ms/step - loss: 0.1315 - accuracy: 0.9597 - val_loss: 0.0739 - val_accuracy: 0.9882\n",
      "Epoch 9/75\n",
      "950/950 [==============================] - 5s 5ms/step - loss: 0.1156 - accuracy: 0.9621 - val_loss: 0.0905 - val_accuracy: 0.9810\n",
      "Epoch 10/75\n",
      "950/950 [==============================] - 4s 5ms/step - loss: 0.1161 - accuracy: 0.9616 - val_loss: 0.0837 - val_accuracy: 0.9763\n",
      "Epoch 11/75\n",
      "950/950 [==============================] - 4s 4ms/step - loss: 0.0992 - accuracy: 0.9705 - val_loss: 0.0593 - val_accuracy: 0.9834\n",
      "Epoch 12/75\n",
      "950/950 [==============================] - 4s 4ms/step - loss: 0.0631 - accuracy: 0.9789 - val_loss: 0.0863 - val_accuracy: 0.9882\n",
      "Epoch 13/75\n",
      "950/950 [==============================] - 4s 4ms/step - loss: 0.0877 - accuracy: 0.9729 - val_loss: 0.0674 - val_accuracy: 0.9787\n",
      "Epoch 14/75\n",
      "950/950 [==============================] - 4s 4ms/step - loss: 0.0765 - accuracy: 0.9787 - val_loss: 0.1244 - val_accuracy: 0.9621\n",
      "Epoch 15/75\n",
      "950/950 [==============================] - 6s 6ms/step - loss: 0.0719 - accuracy: 0.9763 - val_loss: 0.0854 - val_accuracy: 0.9810\n",
      "Epoch 16/75\n",
      "950/950 [==============================] - 4s 5ms/step - loss: 0.0801 - accuracy: 0.9763 - val_loss: 0.0508 - val_accuracy: 0.9882\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x205c4ad2730>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping=EarlyStopping(monitor='val_accuracy', min_delta=0,\n",
    "                            patience=8, verbose=0, mode='max',\n",
    "                            baseline=None, restore_best_weights=True)\n",
    "\n",
    "model_conf.fit(train_X_conf.reshape(-1, 128, 128, 1), train_Y_oh_conf, \n",
    "                epochs=75, batch_size=4, shuffle=True,\n",
    "                validation_data=(val_X_conf.reshape(-1, 128, 128, 1), val_Y_oh_conf),\n",
    "                callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/132 [==============================] - 1s 3ms/step\n",
      "25/25 [==============================] - 0s 4ms/step\n",
      "132/132 [==============================] - 1s 4ms/step\n",
      "25/25 [==============================] - 0s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "Y_pred_conf_train = model_conf.predict(trainVal_X_conf.reshape(-1, 128, 128, 1))\n",
    "Y_pred_conf_test = model_conf.predict(test_X_conf.reshape(-1, 128, 128, 1))\n",
    "\n",
    "Y_pred_deconf_train = model_conf.predict(trainVal_X_deconf.reshape(-1, 128, 128, 1))\n",
    "Y_pred_deconf_test = model_conf.predict(test_X_deconf.reshape(-1, 128, 128, 1))\n",
    "\n",
    "Y_pred_conf_train = np.argmax(Y_pred_conf_train, axis=1)  \n",
    "Y_pred_conf_test = np.argmax(Y_pred_conf_test, axis=1)\n",
    "Y_pred_deconf_train = np.argmax(Y_pred_deconf_train, axis=1)\n",
    "Y_pred_deconf_test = np.argmax(Y_pred_deconf_test, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on confounded test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.93      0.95       152\n",
      "           1       1.00      1.00      1.00       269\n",
      "           2       0.98      0.66      0.79        95\n",
      "           3       0.70      1.00      0.82        74\n",
      "           4       0.94      0.94      0.94       100\n",
      "           5       0.97      1.00      0.98        90\n",
      "\n",
      "    accuracy                           0.94       780\n",
      "   macro avg       0.92      0.92      0.91       780\n",
      "weighted avg       0.95      0.94      0.94       780\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_Y_conf.reshape(-1), Y_pred_conf_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test on deconfounded test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.88      0.91       152\n",
      "         1.0       0.93      0.39      0.55       269\n",
      "         2.0       0.24      0.54      0.33        95\n",
      "         3.0       0.58      0.46      0.51        74\n",
      "         4.0       0.79      0.94      0.86       100\n",
      "         5.0       0.67      1.00      0.80        90\n",
      "\n",
      "    accuracy                           0.65       780\n",
      "   macro avg       0.69      0.70      0.66       780\n",
      "weighted avg       0.77      0.65      0.66       780\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test_Y_deconf.reshape(-1), Y_pred_deconf_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
